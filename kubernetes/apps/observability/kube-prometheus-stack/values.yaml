kube-prometheus-stack:
  fullnameOverride: kube-prometheus-stack
  crds:
    enabled: true
  cleanPrometheusOperatorObjectNames: true
  alertmanager:
    route:
      main:
        enabled: true
        hostnames:
          - alertmanager.int.blarc.my.id
        parentRefs:
          - name: gateway-internal
            namespace: gateway
    alertmanagerSpec:
      alertmanagerConfiguration:
        name: alertmanager
      externalUrl: https://alertmanager.int.blarc.my.id
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: topolvm-thin
            resources:
              requests:
                storage: 1Gi
  kubeApiServer:
    serviceMonitor:
      matchLabels:
        k8s-app: kube-apiserver
  kubeScheduler:
    service:
      matchLabels:
        k8s-app: kube-scheduler
  kubeControllerManager:
    service:
      matchLabels:
        k8s-app: kube-controller-manager
  kubeEtcd:
    enabled: false
  kubeProxy:
    enabled: false
  prometheus:
    route:
      main:
        enabled: true
        parentRefs:
          - name: gateway-internal
            namespace: gateway
        hostnames:
          - "prometheus.int.blarc.my.id"
    prometheusSpec:
      externalUrl: https://prometheus.int.blarc.my.id
      podMonitorSelectorNilUsesHelmValues: false
      probeSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      scrapeConfigSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      enableAdminAPI: true
      walCompression: true
      #        enableFeatures:
      #          - memory-snapshot-on-shutdown
      #        retention: 14d
      #        retentionSize: 50GB
      resources:
        requests:
          cpu: 100m
        limits:
          memory: 2000Mi
  #        storageSpec:
  #          volumeClaimTemplate:
  #            spec:
  #              storageClassName: ceph-block
  #              resources:
  #                requests:
  #                  storage: 50Gi
      additionalScrapeConfigs:
        - job_name: "node-exporter"
          static_configs:
            - targets: [ '192.168.1.13:9100' ]
  prometheus-node-exporter:
    fullnameOverride: node-exporter
    prometheus:
      monitor:
        enabled: true
        relabelings:
          - action: replace
            regex: (.*)
            replacement: $1
            sourceLabels: ["__meta_kubernetes_pod_node_name"]
            targetLabel: kubernetes_node
  kube-state-metrics:
    fullnameOverride: kube-state-metrics
    metricLabelsAllowlist:
      - pods=[*]
      - deployments=[*]
      - persistentvolumeclaims=[*]
    prometheus:
      monitor:
        enabled: true
        relabelings:
          - action: replace
            regex: (.*)
            replacement: $1
            sourceLabels: ["__meta_kubernetes_pod_node_name"]
            targetLabel: kubernetes_node
  grafana:
    enabled: false
    forceDeployDashboards: true
  additionalPrometheusRulesMap:
    dockerhub-rules:
      groups:
        - name: dockerhub
          rules:
            - alert: DockerhubRateLimitRisk
              annotations:
                summary: Kubernetes cluster Dockerhub rate limit risk
              expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
              labels:
                severity: critical
    oom-rules:
      groups:
        - name: oom
          rules:
            - alert: OomKilled
              annotations:
                summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
              expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
              labels:
                severity: critical
  #      zfs-rules:
  #        groups:
  #          - name: zfs
  #            rules:
  #              - alert: ZfsUnexpectedPoolState
  #                annotations:
  #                  summary: ZFS pool {{$labels.zpool}} on {{$labels.instance}} is in a unexpected state {{$labels.state}}
  #                expr: node_zfs_zpool_state{state!="online"} > 0
  #                labels:
  #                  severity: critical
